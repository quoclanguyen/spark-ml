{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đồ án học máy sử dụng Apache Spark ML và SparkSQL\n",
    "* Đồ án này sử dụng pyspark phiên bản 3.4.0 và được chạy trên Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nhập dữ liệu và mô tả dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dữ liệu sử dụng một lược đồ tạo thủ công\n",
    "* Trong notebook này, dữ liệu sử dụng đó là dữ liệu chứa chi tiết của các chuyến bay\n",
    "* Ở những bước đầu tiên, nhóm thực hiện khám phá dữ liệu sau khi load nó vào DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkMLExample').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|\n",
      "|        19|        5|     DL|          10397|        15016|      -1|     -19|\n",
      "|        19|        5|     DL|          15016|        10397|       0|      -1|\n",
      "|        19|        5|     DL|          10397|        14869|      15|      24|\n",
      "|        19|        5|     DL|          10397|        10423|      33|      34|\n",
      "|        19|        5|     DL|          11278|        10397|     323|     322|\n",
      "|        19|        5|     DL|          14107|        13487|      -7|     -13|\n",
      "|        19|        5|     DL|          11433|        11298|      22|      41|\n",
      "|        19|        5|     DL|          11298|        11433|      40|      20|\n",
      "|        19|        5|     DL|          11433|        12892|      -2|      -7|\n",
      "|        19|        5|     DL|          10397|        12451|      71|      75|\n",
      "|        19|        5|     DL|          12451|        10397|      75|      57|\n",
      "|        19|        5|     DL|          12953|        10397|      -1|      10|\n",
      "|        19|        5|     DL|          11433|        12953|      -3|     -10|\n",
      "|        19|        5|     DL|          10397|        14771|      31|      38|\n",
      "|        19|        5|     DL|          13204|        10397|       8|      25|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the dataframe sql data types\n",
    "from pyspark.sql.types import *\n",
    "#\n",
    "# flightSchema describes the structure of the data in the flights.csv file\n",
    "#\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "#\n",
    "# Use the dataframe reader to read the file and \n",
    "#\n",
    "flights = spark.read.csv('data/raw-flight-data.csv', schema=flightSchema, header=True)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dữ liệu sử dụng tính năng tự động tạo lược đồ\n",
    "* Nếu không định nghĩa sẵn lược đồ, có thể cho Spark đọc file và tạo schema tự động\n",
    "* Để minh hoạ, nhóm chọn tập dữ liệu `airports.csv` vì tính đơn giản của nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "|     10304|      Aniak|   AK|       Aniak Airport|\n",
      "|     10754|     Barrow|   AK|Wiley Post/Will R...|\n",
      "|     10551|     Bethel|   AK|      Bethel Airport|\n",
      "|     10926|    Cordova|   AK|Merle K Mudhole S...|\n",
      "|     14709|  Deadhorse|   AK|   Deadhorse Airport|\n",
      "|     11336| Dillingham|   AK|  Dillingham Airport|\n",
      "|     11630|  Fairbanks|   AK|Fairbanks Interna...|\n",
      "|     11997|   Gustavus|   AK|    Gustavus Airport|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read.csv('data/airports.csv', header=True, inferSchema=True)\n",
    "airports.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lược đồ được tạo tự động từ Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_id: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the inferred schema for the airports dataframe\n",
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng các method có sẵn trong DataFrame\n",
    "Spark DataFrames cung cấp nhiều hàm có sẵn dùng để trích xuất và xử lý dữ liệu.  \n",
    "Dưới đây là ví dụ dùng để hiển thị 5 thành phố đầu tiên trong tập dữ liệu về sân bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|       city|                name|\n",
      "+-----------+--------------------+\n",
      "|Adak Island|                Adak|\n",
      "|  Anchorage|Ted Stevens Ancho...|\n",
      "|      Aniak|       Aniak Airport|\n",
      "|     Barrow|Wiley Post/Will R...|\n",
      "|     Bethel|      Bethel Airport|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities = airports.select(\"city\", \"name\")\n",
    "cities.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Operations\n",
    "You can combine functions in a single statement to perform multiple operations on a DataFrame. \n",
    "\n",
    "In this case, we will use the **join** function to combine the **flights** and **airports** DataFrames, and then use the **groupBy** and **count** functions to return the number of flights from each airport.\n",
    "\n",
    "We will then sort by the descending counts to get the top 5 airports by number of flights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|             city| Count|\n",
      "+-----------------+------+\n",
      "|          Chicago|177845|\n",
      "|          Atlanta|149970|\n",
      "|      Los Angeles|118684|\n",
      "|         New York|118540|\n",
      "|Dallas/Fort Worth|105024|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "flightsByOrigin = flights\\\n",
    ".join(airports, flights.OriginAirportID == airports.airport_id)\\\n",
    ".groupBy(\"city\")\\\n",
    ".agg(F.count(F.lit(1)).alias(\"Count\"))\\\n",
    ".orderBy(\"Count\", ascending=False)\n",
    "\n",
    "flightsByOrigin.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Summary Statistics\n",
    "\n",
    "Predictive modeling is based on statistics and probability, so you will often start by looking at summary statistics. \n",
    "\n",
    "The **describe** function returns a DataFrame containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column.\n",
    "\n",
    "Let's look at the flights dataframe statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       DayofMonth|         DayOfWeek|Carrier|   OriginAirportID|     DestAirportID|          DepDelay|         ArrDelay|\n",
      "+-------+-----------------+------------------+-------+------------------+------------------+------------------+-----------------+\n",
      "|  count|          2719418|           2719418|2719418|           2719418|           2719418|           2691974|          2690385|\n",
      "|   mean|15.79747468024408|3.8983907586108497|   null| 12742.26441172339|12742.455345592329| 10.53686662649788| 6.63768791455498|\n",
      "| stddev| 8.79986016898541|1.9859881390373326|   null|1501.9729397025696|1501.9692528927906|36.099528066431446|38.64881489390091|\n",
      "|    min|                1|                 1|     9E|             10140|             10140|               -63|              -94|\n",
      "|    max|               31|                 7|     YV|             15376|             15376|              1863|             1845|\n",
      "+-------+-----------------+------------------+-------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Làm sạch dữ liệu và khám phá dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the Presence of Duplicates\n",
    "The data you have to work with won't always be perfect - often you'll want to *clean* the data; for example to detect and remove duplicates that might affect your model. \n",
    "\n",
    "You can use the **dropDuplicates** function to create a new DataFrame with the duplicates removed, enabling you to determine how many rows are duplicates of other rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows =  22435\n"
     ]
    }
   ],
   "source": [
    "total_flights = flights.count()\n",
    "unique_flights = flights.dropDuplicates().count()\n",
    "\n",
    "print(\"Number of duplicate rows = \",total_flights - unique_flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values using <code>dropNa</code>\n",
    "\n",
    "As well as determing if duplicates exist in your data, you should detect missing values, and either remove rows containing missing data or replace the missing values with a suitable relacement. \n",
    "\n",
    "The **dropna** function creates a DataFrame with any rows containing missing data removed - you can specify a subset of columns, and whether the row should be removed in *any* or *all* values are missing. You can then use this new DataFrame to determine how many rows contain missing values.\n",
    "\n",
    "Below, we count rows that have missing Arrival or Departure delay values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values (excluding dups) =  46233\n"
     ]
    }
   ],
   "source": [
    "unique_flights_withoutNA =  flights.dropDuplicates()\\\n",
    ".dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()\n",
    "\n",
    "print(\"Missing values (excluding dups) = \", total_flights - unique_flights_withoutNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "\n",
    "Now that you've identified that there are duplicates and missing values, you can clean the data by removing the duplicates and replacing the missing values. \n",
    "\n",
    "The **fillna** function replaces missing values with a specified replacement value. \n",
    "\n",
    "In this case, we'll remove all duplicate rows and replace missing **ArrDelay** and **DepDelay** values with **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cleaned data set =  2696983 Number of partitions =  32\n"
     ]
    }
   ],
   "source": [
    "data = flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"]).repartition(32)\n",
    "\n",
    "# Let's cache this for efficient future use\n",
    "data.cache()\n",
    "\n",
    "print(\"Number of rows in cleaned data set = \", data.count(), \"Number of partitions = \", data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Summary Statistics\n",
    "\n",
    "After cleaning the data, we should re-check the statistics - removing rows and changing values may affect the distribution of the data, which in turn could affect any predictive models you might create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|        DayofMonth|         DayOfWeek|Carrier|   OriginAirportID|    DestAirportID|          DepDelay|          ArrDelay|\n",
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           2696983|           2696983|2696983|           2696983|          2696983|           2696983|           2696983|\n",
      "|   mean|15.798996508320593| 3.900369412784582|   null|12742.459424846207|12742.85937657004|10.531134234068217|6.6679285705545785|\n",
      "| stddev| 8.801267199135454|1.9864582421701988|   null|1502.0359941370607|1501.993958981797| 36.06172819056576|38.583861473580725|\n",
      "|    min|                 1|                 1|     9E|             10140|            10140|               -63|               -94|\n",
      "|    max|                31|                 7|     YV|             15376|            15376|              1863|              1845|\n",
      "+-------+------------------+------------------+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Relationships in the Data\n",
    "\n",
    "Predictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another and identify any apparent correlation. \n",
    "\n",
    "The **corr** function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a string negative correlation (near -1) indicates that *low* values for one column are often found with *high* values for the other. A correlation near 0 indicates little apparent relationship between the fields.\n",
    "\n",
    "Let's look at the correlation between *DepDelay* and *ArrDelay*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392630367706979"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr(\"DepDelay\", \"ArrDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark SQL\n",
    "\n",
    "In addition to using the DataFrame API directly to query data, you can persist DataFrames as a table and use Spark SQL to query them using the SQL language. SQL is often more intuitive to use when querying tabular data structures.\n",
    "\n",
    "Here we'll create a view of the cleaned data and run a SQL query to get the average arrival delay by day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|DayOfWeek|Avg Delay(min)|\n",
      "+---------+--------------+\n",
      "|        1|          7.08|\n",
      "|        2|          4.39|\n",
      "|        3|          7.23|\n",
      "|        4|         10.78|\n",
      "|        5|          8.71|\n",
      "|        6|          2.14|\n",
      "|        7|          5.25|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"flightData\")\n",
    "spark.sql(\"\"\" \n",
    "SELECT DayOfWeek, CAST(AVG(ArrDelay) as DECIMAL(6,2)) AS `Avg Delay(min)` \n",
    "FROM flightData \n",
    "GROUP BY DayOfWeek \n",
    "ORDER BY DayOfWeek \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chuẩn bị dữ liệu, xây dựng pipeline và huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Preparing the data for machine learning\n",
    "\n",
    "In this example, the data has already been cleaned. A subset of columns is selected to use as features. The target variable is created as a Boolean label field named Late with **the value 1 for flights that arrived 15 minutes or more after the scheduled arrival time, or 0 if the flight was early or on-time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import sql functions and ML libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Carrier: string (nullable = true)\n",
      " |-- OriginAirportID: integer (nullable = true)\n",
      " |-- DestAirportID: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets\n",
    "\n",
    "It is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. \n",
    "\n",
    "In this exercise, 70% of the data is used for training, and reserve 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows count: 1886184  Testing rows count: 810799\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "train = splits[0]\n",
    "# rename the target variable in the test set to trueLabel\n",
    "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "\n",
    "print (\"Training rows count:\", train_rows, \" Testing rows count:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n",
      " DayofMonth      | 1     \n",
      " DayOfWeek       | 1     \n",
      " Carrier         | 9E    \n",
      " OriginAirportID | 10423 \n",
      " DestAirportID   | 11433 \n",
      " DepDelay        | -5    \n",
      " label           | 0     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training data for SparkML\n",
    "\n",
    "A predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between *continuous features* (which have a calculable numeric value) and *categorical features* (which are numeric representations of discrete categories). It is also common to normalize continuous numeric features to use a common scale (for example, by scaling all numbers to a proportional decimal value between 0 and 1).\n",
    "\n",
    "A pipeline consists of a series of **transformer** and **estimator** stages that typically prepare a DataFrame for modeling and then train a predictive model. \n",
    "\n",
    "In this case, we will create a pipeline with seven stages:\n",
    "1. **StringIndexer** estimator that converts string values to indexes for categorical features\n",
    "2. **VectorAssembler** that combines categorical features into a single vector\n",
    "3. **VectorIndexer** that creates indexes for a vector of categorical features\n",
    "4. **VectorAssembler** that creates a vector of continuous numeric features\n",
    "5. **MinMaxScaler** that normalizes continuous numeric features\n",
    "6. **VectorAssembler** that creates a vector of categorical and continuous features\n",
    "7. **LogisticRegression** classifier that trains a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Stage 1. convert string values to indexes for categorical features\n",
    "strIdx = StringIndexer(inputCol = \"Carrier\", outputCol = \"CarrierIdx\")\n",
    "\n",
    "#Stage 2. combine categorical features into a single vector\n",
    "catVect = VectorAssembler(inputCols = [\"CarrierIdx\", \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\"], outputCol=\"catFeatures\")\n",
    "\n",
    "#Stage 3. create indexes for a vector of categorical features\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "\n",
    "#Stage 4. create a vector of continuous numeric features\n",
    "numVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\n",
    "\n",
    "#Stage 5. normalize continuous numeric features\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "\n",
    "#Stage 6. creates a vector of categorical and continuous features\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
    "\n",
    "#Stage 7. LogisticRegression classifier that trains a classification model\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "\n",
    "# Now define the pipeline\n",
    "pipeline = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " DayofMonth      | 1                                                   \n",
      " DayOfWeek       | 7                                                   \n",
      " Carrier         | 9E                                                  \n",
      " OriginAirportID | 10423                                               \n",
      " DestAirportID   | 11433                                               \n",
      " DepDelay        | -2                                                  \n",
      " label           | 0                                                   \n",
      " CarrierIdx      | 10.0                                                \n",
      " catFeatures     | [10.0,1.0,7.0,10423.0,11433.0]                      \n",
      " idxCatFeatures  | [10.0,1.0,6.0,10423.0,11433.0]                      \n",
      " numFeatures     | [-2.0]                                              \n",
      " normFeatures    | [0.036244800950683304]                              \n",
      " features        | [10.0,1.0,6.0,10423.0,11433.0,0.036244800950683304] \n",
      " rawPrediction   | [1.604044904797468,-1.604044904797468]              \n",
      " probability     | [0.8325829573858674,0.1674170426141326]             \n",
      " prediction      | 0.0                                                 \n",
      "-RECORD 1--------------------------------------------------------------\n",
      " DayofMonth      | 1                                                   \n",
      " DayOfWeek       | 7                                                   \n",
      " Carrier         | 9E                                                  \n",
      " OriginAirportID | 11433                                               \n",
      " DestAirportID   | 13244                                               \n",
      " DepDelay        | -3                                                  \n",
      " label           | 0                                                   \n",
      " CarrierIdx      | 10.0                                                \n",
      " catFeatures     | [10.0,1.0,7.0,11433.0,13244.0]                      \n",
      " idxCatFeatures  | [10.0,1.0,6.0,11433.0,13244.0]                      \n",
      " numFeatures     | [-3.0]                                              \n",
      " normFeatures    | [0.035650623885918005]                              \n",
      " features        | [10.0,1.0,6.0,11433.0,13244.0,0.035650623885918005] \n",
      " rawPrediction   | [1.628354555346268,-1.628354555346268]              \n",
      " probability     | [0.835944104653382,0.16405589534661802]             \n",
      " prediction      | 0.0                                                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "piplineModel.transform(train).filter(\"DayOfWeek == 7\").show(2, vertical=True, truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Classification Model\n",
    "\n",
    "Next, you need to train the classification model. \n",
    "\n",
    "The pipeline itself is an estimator, and so it has a fit method that you can call to run the pipeline on a specified DataFrame. \n",
    "\n",
    "In this case, we will run the pipeline on the training data to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete in: 12.12018579999949 secs\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "piplineModel = pipeline.fit(train)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print (\"Model training complete in:\", elapsed, \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Pipeline Model\n",
    "\n",
    "The model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. We can use this approach to predict delay status for flights where the label is unknown; but in this case we are using the test data which includes a known true label value, so you can compare the predicted status to the actual status.\n",
    "\n",
    "Let's transform the **test** DataFrame using the pipeline to generate label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------+---------+\n",
      "|features                                           |prediction|trueLabel|\n",
      "+---------------------------------------------------+----------+---------+\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.033273915626856804]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.035056446821152706]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14100.0,0.06833036244800951] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,13244.0,11193.0,0.035056446821152706]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,11193.0,0.033273915626856804]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11193.0,0.03683897801544861] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,14730.0,0.04278074866310161] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,14122.0,11433.0,0.0677361853832442]  |0.0       |1        |\n",
      "|[2.0,1.0,0.0,10397.0,11298.0,0.035056446821152706] |0.0       |0        |\n",
      "|[2.0,1.0,0.0,10821.0,11298.0,0.03149138443256091]  |0.0       |0        |\n",
      "+---------------------------------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = piplineModel.transform(test)\n",
    "predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n",
    "predicted.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, the prediction column contains the predicted value for the label, and the trueLabel column contains the actual known value from the testing data. It looks like there are a mix of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Đánh giá kết quả và finetune mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the classifier: Compute Confusion Matrix Metrics\n",
    "\n",
    "Classifiers are typically evaluated by creating a confusion matrix, which indicates the number of:\n",
    "\n",
    "True Positives\n",
    "\n",
    "True Negatives\n",
    "\n",
    "False Positives\n",
    "\n",
    "False Negatives\n",
    "\n",
    "From these core measures, other evaluation metrics such as precision and recall can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(tp, fp, tn, fn):\n",
    "    print(f\"TP = {tp}\")\n",
    "    print(f\"FP = {fp}\")\n",
    "    print(f\"TN = {tn}\")\n",
    "    print(f\"FN = {fn}\")\n",
    "    print(f\"Precision = {tp / (tp + fp)}\")\n",
    "    print(f\"Recall = {tp / (tp + fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 19439.0\n",
      "FP = 77.0\n",
      "TN = 648611.0\n",
      "FN = 142672.0\n",
      "Precision = 0.9960545193687231\n",
      "Recall = 0.11991166546378716\n"
     ]
    }
   ],
   "source": [
    "tp = float(predicted.filter(\"prediction == 1.0 AND trueLabel == 1\").count())\n",
    "fp = float(predicted.filter(\"prediction == 1.0 AND trueLabel == 0\").count())\n",
    "tn = float(predicted.filter(\"prediction == 0.0 AND trueLabel == 0\").count())\n",
    "fn = float(predicted.filter(\"prediction == 0.0 AND trueLabel == 1\").count())\n",
    "show_metrics(tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### View the Raw Prediction and Probability\n",
    "\n",
    "The prediction is based on a raw prediction score that describes a labeled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "|rawPrediction                           |probability                             |prediction|trueLabel|\n",
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "|[1.6608635911522716,-1.6608635911522716]|[0.8403538958333158,0.15964610416668423]|0.0       |0        |\n",
      "|[1.6271287650280646,-1.6271287650280646]|[0.8357759286253585,0.16422407137464146]|0.0       |0        |\n",
      "|[0.846317950913954,-0.846317950913954]  |[0.6997941786898367,0.30020582131016327]|0.0       |1        |\n",
      "|[1.6337358540744185,-1.6337358540744185]|[0.8366807708136917,0.16331922918630826]|0.0       |1        |\n",
      "|[1.6778103740905967,-1.6778103740905967]|[0.8426143708076425,0.15738562919235755]|0.0       |0        |\n",
      "|[1.5938985231552538,-1.5938985231552538]|[0.8311638912087153,0.16883610879128474]|0.0       |1        |\n",
      "|[1.4570114813374075,-1.4570114813374075]|[0.8110751630365467,0.1889248369634533] |0.0       |0        |\n",
      "|[0.8723999849530804,-0.8723999849530804]|[0.7052448394341079,0.2947551605658921] |0.0       |1        |\n",
      "|[1.6107607655768832,-1.6107607655768832]|[0.8335169819296755,0.1664830180703245] |0.0       |0        |\n",
      "|[1.6983692588520078,-1.6983692588520078]|[0.8453216307184838,0.15467836928151624]|0.0       |0        |\n",
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\")\\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results include rows where the probability for 0 (the first value in the probability vector) is only slightly higher than the probability for 1 (the second value in the probability vector). The default discrimination threshold (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the Area Under ROC\n",
    "\n",
    "Another way to assess the performance of a classification model is to measure the area under a ROC curve for the model. The spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this. \n",
    "\n",
    "The ROC curve shows the True Positive and False Positive rates plotted for varying thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve =  0.9233582145600991\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "aur = evaluator.evaluate(prediction)\n",
    "print (\"Area under the ROC curve = \", aur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the Discrimination Threshold\n",
    "\n",
    "The AUC score seems to indicate a reasonably good model, but the performance metrics seem to indicate that it predicts a high number of False Negative labels (i.e. it predicts 0 when the true label is 1), leading to a low Recall. \n",
    "\n",
    "You can affect the way a model performs by changing its parameters. For example, as noted previously, the default discrimination threshold is set to 0.5 - so if there are a lot of False Positives, you may want to consider raising this; or conversely, you may want to address a large number of False Negatives by lowering the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "|rawPrediction                           |probability                             |prediction|trueLabel|\n",
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "|[1.660863591152272,-1.660863591152272]  |[0.8403538958333158,0.15964610416668423]|0.0       |0        |\n",
      "|[1.627128765028065,-1.627128765028065]  |[0.8357759286253587,0.16422407137464134]|0.0       |0        |\n",
      "|[0.8463179509139545,-0.8463179509139545]|[0.6997941786898368,0.30020582131016316]|0.0       |1        |\n",
      "|[1.633735854074419,-1.633735854074419]  |[0.8366807708136917,0.16331922918630826]|0.0       |1        |\n",
      "|[1.6778103740905972,-1.6778103740905972]|[0.8426143708076426,0.15738562919235743]|0.0       |0        |\n",
      "|[1.5938985231552543,-1.5938985231552543]|[0.8311638912087153,0.16883610879128474]|0.0       |1        |\n",
      "|[1.4570114813374078,-1.4570114813374078]|[0.8110751630365467,0.1889248369634533] |0.0       |0        |\n",
      "|[0.8723999849530808,-0.8723999849530808]|[0.705244839434108,0.294755160565892]   |0.0       |1        |\n",
      "|[1.6107607655768836,-1.6107607655768836]|[0.8335169819296756,0.1664830180703244] |0.0       |0        |\n",
      "|[1.6983692588520083,-1.6983692588520083]|[0.8453216307184838,0.15467836928151624]|0.0       |0        |\n",
      "+----------------------------------------+----------------------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change the threshold to 0.3 and create a new LogisticRegression model\n",
    "lr2 = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3, threshold=0.35)\n",
    "\n",
    "#Set up new pipeline\n",
    "pipeline2 = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr2])\n",
    "model2 = pipeline2.fit(train)\n",
    "\n",
    "#Make new predictions\n",
    "newPrediction = model2.transform(test)\n",
    "newPrediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\")\\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 42033.0\n",
      "FP = 120.0\n",
      "TN = 648568.0\n",
      "FN = 120078.0\n",
      "Precision = 0.9971532275282898\n",
      "Recall = 0.259285304513574\n"
     ]
    }
   ],
   "source": [
    "# Recalculate confusion matrix, using the new predictions\n",
    "tp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "\n",
    "show_metrics(tp2, fp2, tn2, fn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are now more True Positives and less False Negatives, and Recall has improved. By changing the discrimination threshold, the model now gets more predictions correct - though it's worth noting that the number of False Positives has also increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Parameters using cross validation with grid search\n",
    "\n",
    "You can tune parameters to find the best model for your data. To do this we can use the **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple folds of the data split into training and validation datasets, in order to find the best performing parameters. \n",
    "\n",
    "* Note that this can take a long time to run because every parameter combination is tried multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(lr.regParam, [0.3])\\\n",
    ".addGrid(lr.maxIter, [10])\\\n",
    ".addGrid(lr.threshold, [0.25, 0.3, 0.35])\\\n",
    ".build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=BinaryClassificationEvaluator(),\\\n",
    "                    estimatorParamMaps=paramGrid, numFolds=5)\n",
    "\n",
    "modelCV = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Model\n",
    "\n",
    "Now we're ready to apply the model to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------+---------+\n",
      "|features                                           |prediction|trueLabel|\n",
      "+---------------------------------------------------+----------+---------+\n",
      "|[10.0,1.0,0.0,11433.0,12339.0,0.033273915626856804]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,11278.0,0.035056446821152706]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,12478.0,14100.0,0.06833036244800951] |1.0       |1        |\n",
      "|[10.0,1.0,0.0,13244.0,11193.0,0.035056446821152706]|0.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,11193.0,0.033273915626856804]|0.0       |0        |\n",
      "|[10.0,1.0,0.0,13487.0,11193.0,0.03683897801544861] |0.0       |1        |\n",
      "|[10.0,1.0,0.0,13487.0,14730.0,0.04278074866310161] |0.0       |0        |\n",
      "|[10.0,1.0,0.0,14122.0,11433.0,0.0677361853832442]  |1.0       |1        |\n",
      "|[2.0,1.0,0.0,10397.0,11298.0,0.035056446821152706] |0.0       |0        |\n",
      "|[2.0,1.0,0.0,10821.0,11298.0,0.03149138443256091]  |0.0       |0        |\n",
      "+---------------------------------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionCV = modelCV.transform(test)\n",
    "predictedCV = predictionCV.select(\"features\", \"prediction\", \"trueLabel\")\n",
    "predictedCV.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 86614.0\n",
      "FP = 1472.0\n",
      "TN = 647216.0\n",
      "FN = 75497.0\n",
      "Precision = 0.9832890584201803\n",
      "Recall = 0.5342882346046844\n"
     ]
    }
   ],
   "source": [
    "# Recalculate confusion matrix, using the new predictions\n",
    "tp3 = float(predictionCV.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp3 = float(predictionCV.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn3 = float(predictionCV.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn3 = float(predictionCV.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "\n",
    "show_metrics(tp3, fp3, tn3, fn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the recall metrics has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bestPipeline = modelCV.bestModel\n",
    "bestLRModel = bestPipeline.stages[6]\n",
    "bestParams = bestLRModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  LogisticRegression_2fe46111e484__aggregationDepth  ---> Value =  2\n",
      "Key:  LogisticRegression_2fe46111e484__elasticNetParam  ---> Value =  0.0\n",
      "Key:  LogisticRegression_2fe46111e484__family  ---> Value =  auto\n",
      "Key:  LogisticRegression_2fe46111e484__featuresCol  ---> Value =  features\n",
      "Key:  LogisticRegression_2fe46111e484__fitIntercept  ---> Value =  True\n",
      "Key:  LogisticRegression_2fe46111e484__labelCol  ---> Value =  label\n",
      "Key:  LogisticRegression_2fe46111e484__maxBlockSizeInMB  ---> Value =  0.0\n",
      "Key:  LogisticRegression_2fe46111e484__maxIter  ---> Value =  10\n",
      "Key:  LogisticRegression_2fe46111e484__predictionCol  ---> Value =  prediction\n",
      "Key:  LogisticRegression_2fe46111e484__probabilityCol  ---> Value =  probability\n",
      "Key:  LogisticRegression_2fe46111e484__rawPredictionCol  ---> Value =  rawPrediction\n",
      "Key:  LogisticRegression_2fe46111e484__regParam  ---> Value =  0.3\n",
      "Key:  LogisticRegression_2fe46111e484__standardization  ---> Value =  True\n",
      "Key:  LogisticRegression_2fe46111e484__threshold  ---> Value =  0.25\n",
      "Key:  LogisticRegression_2fe46111e484__tol  ---> Value =  1e-06\n"
     ]
    }
   ],
   "source": [
    "#type(bestParams)\n",
    "for k,v in bestParams.items():\n",
    "    print(\"Key: \", k, \" ---> Value = \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement in recall was obtained with the change in threshold (Original: 0.35, Tuned: 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve =  0.9233577619046763\n"
     ]
    }
   ],
   "source": [
    "eval2 = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "aur2 = eval2.evaluate(predictionCV)\n",
    "print (\"Area under the ROC curve = \", aur2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "No significant reduction in Area under the ROC curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
